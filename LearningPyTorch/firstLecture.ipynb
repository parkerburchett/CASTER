{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# source: https://www.youtube.com/watch?v=BzcBsTou0C0 \r\n",
    "\r\n",
    "Lecture 1\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch is numpy on a gpu with some nice helper funtions\r\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([10.,  3.])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor([5,3])\r\n",
    "y = torch.Tensor([2,1])\r\n",
    "x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., 0., 0.],\n        [0., 0., 0.]])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros([2,3,]) # rows, columns\r\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.rand([5,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.7188, 0.0875, 0.5934, 0.0457, 0.1012, 0.1266, 0.1482, 0.5630, 0.4907,\n         0.9637, 0.9691, 0.4173, 0.9587, 0.6751, 0.2707, 0.8751, 0.4063, 0.3243,\n         0.1056, 0.9458, 0.0360, 0.7773, 0.7259, 0.7688, 0.3765]])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is how you flatten things\r\n",
    "y.view([1,25]) # does not transform in place you would need to reassign\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2 https://www.youtube.com/watch?v=i2yPxY2rOzs\r\n",
    "\r\n",
    "The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision # just has a bunch of vision dataset\r\n",
    "import torch\r\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST(\"\", train=True, download=True, transform = transforms.Compose([transforms.ToTensor()]))\r\n",
    "test = datasets.MNIST(\"\", train=False, download=True, transform = transforms.Compose([transforms.ToTensor()]))\r\n",
    "                  \r\n",
    "\r\n",
    "# the transfrom parameter is a list of functions that will be the transformations of data\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True) # batch says feed 10 items into the model at a time typically between 8 and 64\r\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=True) # just trial and error. You always want to shuffle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([1, 1, 7, 7, 4, 2, 8, 1, 6, 6])]\n"
     ]
    }
   ],
   "source": [
    "for data in trainset:\r\n",
    "    print(data)\r\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "x,y = data[0][0], data[1][0]\r\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x15b02016d30>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": "<Figure size 432x288 with 1 Axes>"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "plt.imshow(x.view([28,28]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  https://www.youtube.com/watch?v=ixathu7U-LQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Net(\n  (fc1): Linear(in_features=784, out_features=64, bias=True)\n  (fc2): Linear(in_features=64, out_features=64, bias=True)\n  (fc3): Linear(in_features=64, out_features=64, bias=True)\n  (fc4): Linear(in_features=64, out_features=10, bias=True)\n)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building the model\r\n",
    "\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "# these two libraries are mostly interchangeable\r\n",
    "\r\n",
    "class Net(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super().__init__()\r\n",
    "        # self. fully connected layer 1 = nn.Linear(input flat tensor (1d array size is 728))\r\n",
    "        self.fc1 = nn.Linear(28*28, 64) # format is input size, output size. You can choose output size\r\n",
    "        self.fc2 = nn.Linear(64, 64)\r\n",
    "        self.fc3 = nn.Linear(64, 64)\r\n",
    "        self.fc4 = nn.Linear(64, 10) # there are 10 classes so this is the end size. \r\n",
    "\r\n",
    "    # this is a normal basic feed forward network\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        \"\"\"\r\n",
    "            Define how data,x, passes through the network\r\n",
    "        \"\"\"\r\n",
    "        # x is the data that you are passign though the network\r\n",
    "\r\n",
    "        x = F.relu(self.fc1(x)) # pass the data x through the first Fully connected node. Then pass easch element through the ReLu activation function\r\n",
    "        x = F.relu(self.fc2(x))\r\n",
    "        x = F.relu(self.fc3(x))\r\n",
    "        x = self.fc4(x) # we want a function that constrains such as argmax\r\n",
    "        return F.log_softmax(x, dim=1) # converts x into a prob distribution of catgories \r\n",
    "net = Net()\r\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 784])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand([28,28])\r\n",
    "#X = X.view([28*28]) # these things are very finicky about how they want you to fomrat things\r\n",
    "X = X.view(-1,28*28)\r\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-2.4246, -2.4057, -2.3352, -2.3110, -2.3440, -2.3057, -2.1970, -2.2579,\n         -2.1524, -2.3249]], grad_fn=<LogSoftmaxBackward>)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = net(X) # this throws an error because X is not flattened\r\n",
    "\r\n",
    "output # you have successfully passed an image through a NN you built"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4\r\n",
    "\r\n",
    "https://www.youtube.com/watch?v=9j-_dOze4IM\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0356, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0147, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2167, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# loss is a measure of how wrong the model is\r\n",
    "# the optimizer is the thing that changes the weights\r\n",
    "\r\n",
    "import torch.optim as optim\r\n",
    "                                        # you also choose the learning rate\r\n",
    "\r\n",
    "optimizer  = optim.Adam(net.parameters(),lr=.001)# first param is everything that is adjustable in the model. You can change it so that they can only change a subset of the layers. \r\n",
    "# this is useful for when you already have a pretrained model that you want to adapt to your own purposes. \r\n",
    "\r\n",
    "# an epoch is a full pass through the data\r\n",
    "\r\n",
    "EPOCHS =3\r\n",
    "\r\n",
    "for epoch in range(3): # 3 full passes over the data\r\n",
    "    for data in trainset:  # `data` is a batch of data\r\n",
    "        X, y = data  # X is the batch of features, y is the batch of targets.\r\n",
    "        net.zero_grad()  # sets gradients to 0 before loss calc. You will do this likely every step.\r\n",
    "        output = net(X.view(-1,784))  # pass in the reshaped batch (recall they are 28x28 atm)\r\n",
    "        loss = F.nll_loss(output, y)  # calc and grab the loss value\r\n",
    "        loss.backward()  # apply this loss backwards thru the network's parameters\r\n",
    "        optimizer.step()  # attempt to optimize weights to account for loss/gradients\r\n",
    "    print(loss)  # print loss. We hope loss (a measure of wrong-ness) declines! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.96"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total =0 \r\n",
    "correct = 0\r\n",
    "\r\n",
    "with torch.no_grad():\r\n",
    "    for data in testset:\r\n",
    "        X,y = data\r\n",
    "        output = net(X.view(-1,784))\r\n",
    "        for idx, i in enumerate(output):\r\n",
    "            if torch.argmax(i) == y[idx]:\r\n",
    "                correct +=1\r\n",
    "            total +=1\r\n",
    "\r\n",
    "round(correct / total ,2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('venv': venv)",
   "name": "pythonjvsc74a57bd04e7892b4f467507d725e34517222c9d31081f1c74175e58fa588dd5bafca0b86"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "4e7892b4f467507d725e34517222c9d31081f1c74175e58fa588dd5bafca0b86"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}